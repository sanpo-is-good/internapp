import streamlit as st
import json
import google.generativeai as genai

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="Personal Database Sync", layout="wide")

# APIåˆæœŸåŒ– (Gemini)
if "GEMINI_API_KEY" in st.secrets:
    genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
    # è»½å¿«ã«å‹•ããƒ¢ãƒ‡ãƒ« "gemini-1.5-flash" ã‚’ä½¿ç”¨
    model = genai.GenerativeModel('gemini-flash-latest')
else:
    model = None
    st.warning("âš ï¸ .streamlit/secrets.toml ã« GEMINI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")

# ==========================================
# ã‚µã‚¤ãƒ‰ãƒãƒ¼: ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ & ãƒ¢ãƒ¼ãƒ‰é¸æŠ
# ==========================================
st.sidebar.header("ğŸ“‚ Database Loader")

# è‡ªåˆ†ã®DB
file_a = st.sidebar.file_uploader("Upload Your Database (User A)", type=["json"], key="file_a")
data_a = None
if file_a:
    data_a = json.load(file_a)
    st.sidebar.success(f"User A: {data_a.get('profile', {}).get('name', 'Loaded')}")

# ç›¸æ‰‹ã®DB
file_b = st.sidebar.file_uploader("Upload Partner's Database (User B)", type=["json"], key="file_b")
data_b = None
if file_b:
    data_b = json.load(file_b)
    st.sidebar.success(f"User B: {data_b.get('profile', {}).get('name', 'Loaded')}")

st.sidebar.divider()
mode = st.sidebar.radio("Select Mode", ["âš¡ SYNC (å…±é€šç‚¹æŠ½å‡º)", "ğŸ¤– NotebookLM (AIå¯¾è©±)"])

st.title("ğŸ§¬ Personal Database Sync")

# ==========================================
# Mode 1: SYNC (å…±é€šç‚¹ã®å‰²ã‚Šå‡ºã—)
# ==========================================
if mode == "âš¡ SYNC (å…±é€šç‚¹æŠ½å‡º)":
    st.header("Resonance Finder")
    st.write("äºŒã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ç…§åˆã—ã€å…±é€šã®è©±é¡Œã‚„éš ã‚ŒãŸæ¥ç‚¹ã‚’æŠ½å‡ºã—ã¾ã™ã€‚")

    if st.button("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ç…§åˆ (SYNC)", type="primary"):
        if not model:
            st.error("APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ã€‚")
        elif not data_a or not data_b:
            st.warning("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰ã€è‡ªåˆ†ã¨ç›¸æ‰‹ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸¡æ–¹ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("Analyzing Context with Gemini..."):
                str_a = json.dumps(data_a, ensure_ascii=False)
                str_b = json.dumps(data_b, ensure_ascii=False)

                # prompt = f"""
                # ã‚ãªãŸã¯é«˜åº¦ãªãƒãƒƒãƒãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã™ã€‚ä»¥ä¸‹ã®2åã®JSONãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€åŒ–å­¦åå¿œãŒèµ·ãã‚‹ãƒã‚¤ãƒ³ãƒˆã‚’æ¢ã£ã¦ãã ã•ã„ã€‚

                # ã€å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘
                # 1. **ã‚³ã‚¢ãƒ»ãƒ¬ã‚¾ãƒŠãƒ³ã‚¹**: [äºŒäººã®æœ€ã‚‚å¼·ã„å…±é€šé …ã‚’ä¸€è¨€ã§]
                # 2. **è©±é¡Œã®ææ¡ˆ**: [å…·ä½“çš„ãªä¼šè©±ãƒã‚¿ã‚’3ã¤]
                # 3. **ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ¡ˆ**: [ã‚‚ã—äºŒäººãŒä½•ã‹ä½œã‚‹ã¨ã—ãŸã‚‰ï¼Ÿ]

                # ---
                # [User A Data]
                # {str_a}

                # [User B Data]
                # {str_b}
                # """
                prompt = f"""
                ä»¥ä¸‹ã¯ã€ã‚ã‚‹å€‹äººã®è„³å†…ã‚’æ§‹æˆã™ã‚‹ã€Œåè©ã®ã‚¿ã‚°ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆbrain_dumpï¼‰ã€ã§ã™ã€‚
                æ–‡è„ˆã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ã“ã‚Œã‚‰ã®å˜èªã®é›†åˆä½“ã‹ã‚‰ã€ã“ã®äººç‰©ã®ã€Œç¾æ„è­˜ã€ã€Œæ€è€ƒã®ç™–ã€ã€Œéš ã‚ŒãŸèˆˆå‘³ã€ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã¦ãã ã•ã„ã€‚

                ãã®ä¸Šã§ã€ç›¸æ‰‹ï¼ˆUser Bï¼‰ã®ã‚¿ã‚°ã‚¯ãƒ©ã‚¦ãƒ‰ã¨æ¯”è¼ƒã—ã€
                ã€Œä¸€è¦‹é–¢ä¿‚ãªã•ãã†ã ãŒã€æ·±å±¤ã§ç¹‹ãŒã£ã¦ã„ã‚‹æ¦‚å¿µï¼ˆCore Resonanceï¼‰ã€ã‚’ç™ºè¦‹ã—ã¦ãã ã•ã„ã€‚

                [User A Brain Dump]
                {json.dumps(data_a['brain_dump'], ensure_ascii=False)}

                [User B Brain Dump]
                {json.dumps(data_b['brain_dump'], ensure_ascii=False)}
                """

                try:
                    response = model.generate_content(prompt)
                    st.success("Analysis Complete.")
                    st.markdown(response.text)
                except Exception as e:
                    st.error(f"Error: {e}")

# ==========================================
# Mode 2: NotebookLM (AIã¨ã®å¯¾è©±)
# ==========================================
elif mode == "ğŸ¤– NotebookLM (AIå¯¾è©±)":
    st.header("Personal AI Chat")
    
    target_option = st.radio(
        "ã©ã®ãƒ‡ãƒ¼ã‚¿ã¨è©±ã—ã¾ã™ã‹ï¼Ÿ", 
        ["User A (è‡ªåˆ†)", "User B (ç›¸æ‰‹)", "Both (äºŒäºº)"], 
        horizontal=True
    )

    target_data_str = ""
    system_role = ""

    if target_option == "User A (è‡ªåˆ†)" and data_a:
        target_data_str = json.dumps(data_a, ensure_ascii=False)
        system_role = "ã‚ãªãŸã¯User Aã®ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ„ã‚¤ãƒ³ã§ã™ã€‚ä¸€äººç§°ã¯ã€ç§ã€ã§ã€User Aã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæœ¬äººã«ãªã‚Šãã£ã¦ç­”ãˆã¦ãã ã•ã„ã€‚"
    elif target_option == "User B (ç›¸æ‰‹)" and data_b:
        target_data_str = json.dumps(data_b, ensure_ascii=False)
        system_role = "ã‚ãªãŸã¯User Bã®ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ„ã‚¤ãƒ³ã§ã™ã€‚ä¸€äººç§°ã¯ã€ç§ã€ã§ã€User Bã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãæœ¬äººã«ãªã‚Šãã£ã¦ç­”ãˆã¦ãã ã•ã„ã€‚"
    elif target_option == "Both (äºŒäºº)" and (data_a and data_b):
        target_data_str = "User A:" + json.dumps(data_a, ensure_ascii=False) + "\nUser B:" + json.dumps(data_b, ensure_ascii=False)
        system_role = "ã‚ãªãŸã¯User Aã¨User Bã®ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’çŸ¥ã‚‹ç®¡ç†è€…ã§ã™ã€‚äºŒäººã®é–¢ä¿‚æ€§ã‚„ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦å®¢è¦³çš„ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
    
    if not target_data_str:
        st.info("ğŸ‘ˆ å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # å±¥æ­´ã‚’è¡¨ç¤º
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    if user_query := st.chat_input("è³ªå•ã‚’å…¥åŠ›..."):
        if not target_data_str:
            st.error("ãƒ‡ãƒ¼ã‚¿ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        elif not model:
            st.error("APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ã€‚")
        else:
            # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›è¡¨ç¤º
            st.session_state.messages.append({"role": "user", "content": user_query})
            with st.chat_message("user"):
                st.markdown(user_query)

            # AIå›ç­”ç”Ÿæˆ
            with st.chat_message("assistant"):
                # Geminiã¯ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦ç®¡ç†ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ãŒã€
                # ç°¡æ˜“çš„ã«æ¯å›ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¸¡ã™æ–¹å¼ã§å®Ÿè£…ã—ã¾ã™
                
                full_prompt = f"""
                {system_role}
                
                ã€å‚ç…§ãƒ‡ãƒ¼ã‚¿ã€‘
                {target_data_str}

                ã€ã“ã‚Œã¾ã§ã®ä¼šè©±ã€‘
                {[m['content'] for m in st.session_state.messages[-5:]]}

                ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã€‘
                {user_query}
                """
                
                try:
                    response = model.generate_content(full_prompt)
                    bot_reply = response.text
                    st.markdown(bot_reply)
                    st.session_state.messages.append({"role": "assistant", "content": bot_reply})
                except Exception as e:
                    st.error(f"Error: {e}")